{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e43ae3",
   "metadata": {},
   "source": [
    "### In-Context Cross-lingual Transfer.\n",
    "Inference example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "from transformers import AutoTokenizer, MT5ForConditionalGeneration, TrainingArguments\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from src.data_handling import get_class_set, get_kshot_dataset, get_class_objects\n",
    "from src.ic_xlt_utils import create_icl_dataset, run_inference, compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03def00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## instance model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-large')\n",
    "\n",
    "base_model = MT5ForConditionalGeneration.from_pretrained('google/mt5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478c6ad",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Also, create One-Shot adaptation dataset from the training set on the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c96ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1-shots per label for train...\n",
      "Length of reduced training dataset: 18\n",
      "Selecting 1-shots per label for train...\n",
      "Length of reduced training dataset: 18\n",
      "Selecting 1-shots per label for train...\n",
      "Length of reduced training dataset: 18\n",
      "Selecting 1-shots per label for train...\n",
      "Length of reduced training dataset: 18\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/massive'\n",
    "\n",
    "## load datasets of target languages and select one-shot from training data\n",
    "\n",
    "target_languages = ['english','azeri','turkish','swahili']\n",
    "\n",
    "tl_datasets = {}\n",
    "\n",
    "for tl in target_languages:\n",
    "    \n",
    "    tl_datasets[tl] = {\n",
    "        'test': Dataset.load_from_disk('/'.join([data_dir,'test',tl])) ## to evaluate\n",
    "    }\n",
    "    \n",
    "    ## load training to retrieve the one-shot demonstration and reduce to one-shot\n",
    "    \n",
    "    tl_datasets[tl]['train'] = get_kshot_dataset(\n",
    "        dataset = Dataset.load_from_disk('/'.join([data_dir,'train',tl])),\n",
    "        k = 1, # One-Shot per label\n",
    "        seed = 42, # Seed to select shots\n",
    "    )\n",
    "    \n",
    "    tl_datasets[tl]['class_set'],tl_datasets[tl]['lbl2id_class'], _ = get_class_objects(tl_datasets[tl]['train'],tl_datasets[tl]['test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b71040",
   "metadata": {},
   "source": [
    "### Inference on target languages / IC-XLT\n",
    "\n",
    "In-Context Cross-lingual Transfer evaluated in a given target language.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b692766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load lora trained with ICT\n",
    "\n",
    "path_lora_ict = 'trained_loras/massive/ict_m10/' # or 'trained_loras/acd/ict_m10/' to load the ACD trained model\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model,path_lora_ict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fcbe7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on english\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b001330b114f2c8fe6d327b0681506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on azeri\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfc950759934c90aac18b11f5b3d23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on turkish\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95f1b3d65a74e779bae8c058ea384d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on swahili\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233a15ac79c541baa6d548967910c65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## iterate and predict/evaluate over target languages\n",
    "metrics_per_lang = {}\n",
    "\n",
    "for tl in target_languages:\n",
    "    \n",
    "    print(f'Running inference on {tl}')\n",
    "\n",
    "    ## prepend one-shot demonstration in context\n",
    "    icl_dataset_test = create_icl_dataset(dataset_test = tl_datasets[tl]['test'], dataset_train = tl_datasets[tl]['train'])\n",
    "    \n",
    "    ## predict samples\n",
    "    predicted_icxlt = run_inference(\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                test_texts = icl_dataset_test['text'],\n",
    "                class_set = tl_datasets[tl]['class_set'],\n",
    "            )\n",
    "    \n",
    "    ## evaluate samples\n",
    "    metrics_per_lang[tl] = compute_metrics(\n",
    "        predicted_labels = predicted_icxlt,\n",
    "        truth_labels = tl_datasets[tl]['test']['label'],\n",
    "        class_set = tl_datasets[tl]['class_set'],\n",
    "        lbl2id_class = tl_datasets[tl]['lbl2id_class']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e804cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC-XLT\n",
      "F1 micro in target languages (with English as source)\n",
      "\tenglish : 0.8920645595158037\n",
      "\tazeri : 0.8140551445864156\n",
      "\tturkish : 0.8437867832520598\n",
      "\tswahili : 0.7915265635507733\n"
     ]
    }
   ],
   "source": [
    "print('IC-XLT\\nF1 micro in target languages (with English as source)')\n",
    "for tl in target_languages:\n",
    "    print('\\t{} : {}'.format(tl,metrics_per_lang[tl]['f1_score_micro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b168c77",
   "metadata": {},
   "source": [
    "### Inference on target languages / ZS-XLT\n",
    "\n",
    "Zero/Few Cross-lingual Transfer (fine-tuned) evaluated in a given target language.<br>\n",
    "To evaluate 1S/8S-XLT, we just continue fine-tuning the checkpoint with the reduced training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fde843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model trained with prompt-based fine-tuning \n",
    "path_lora_pft = 'trained_loras/massive/pft/' # or 'trained_loras/acd/pbt/' to load the ACD trained model\n",
    "model = PeftModel.from_pretrained(base_model,path_lora_pft) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41af4da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on english\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d11dabf46044c32a57b4ba588487903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on azeri\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a9771aab004fe3912d9d756e4a9ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on turkish\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c32ba8ebb00411b92227eb3bfdca0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on swahili\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e857207827f4b858fcf8b01ce31b4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/est_licenciatura_e.villacueva/anaconda3/envs/dl_bnb/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## iterate and predict/evaluate over target languages\n",
    "\n",
    "metrics_per_lang = {}\n",
    "\n",
    "for tl in target_languages:\n",
    "    \n",
    "    print(f'Running inference on {tl}')\n",
    "\n",
    "    ## predict samples\n",
    "    predicted_zsxlt = run_inference(\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                test_texts = tl_datasets[tl]['test']['text'],\n",
    "                class_set = tl_datasets[tl]['class_set'],\n",
    "            )\n",
    "    \n",
    "    ## evaluate samples\n",
    "    metrics_per_lang[tl] = compute_metrics(\n",
    "        predicted_labels = predicted_zsxlt,\n",
    "        truth_labels = tl_datasets[tl]['test']['label'],\n",
    "        class_set = tl_datasets[tl]['class_set'],\n",
    "        lbl2id_class = tl_datasets[tl]['lbl2id_class']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4d15dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZS-XLT\n",
      "F1 micro in target languages (with English as source)\n",
      "\tenglish : 0.8954270342972428\n",
      "\tazeri : 0.7007397444519166\n",
      "\tturkish : 0.7683254875588433\n",
      "\tswahili : 0.6395427034297243\n"
     ]
    }
   ],
   "source": [
    "print('ZS-XLT\\nF1 micro in target languages (with English as source)')\n",
    "for tl in target_languages:\n",
    "    print('\\t{} : {}'.format(tl,metrics_per_lang[tl]['f1_score_micro']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752ddcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
