{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7be3768",
   "metadata": {},
   "source": [
    "### In-Context Cross-lingual Transfer.\n",
    "Training example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REMOVE LATER\n",
    "## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## \n",
    "'''\n",
    "This cell is needed to make the trainer work in HPC notebooks, if it is not used a weird error is raised.\n",
    "'''\n",
    "import os\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9996\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## \n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, MT5ForConditionalGeneration, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "from src.data_handling import get_class_objects\n",
    "from src.ic_xlt_utils import train_lora, preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22eaf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set source language\n",
    "source_language = 'english'\n",
    "\n",
    "## load data\n",
    "data_dir = 'data/massive' ## or 'data/acd'\n",
    "\n",
    "## convert to transformer Dataset object\n",
    "dataset_train = Dataset.load_from_disk('/'.join([data_dir,'train',source_language]))\n",
    "dataset_test = Dataset.load_from_disk('/'.join([data_dir,'test',source_language]))\n",
    "\n",
    "## retrieve useful variables\n",
    "class_set,lbl2id_class, id2lbl_class = get_class_objects(dataset_train,dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a8d6b",
   "metadata": {},
   "source": [
    "We employ an mT5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import model and tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-large')\n",
    "base_model = MT5ForConditionalGeneration.from_pretrained('google/mt5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e17994",
   "metadata": {},
   "source": [
    "$M$ is the number of examples prepended to the context.<br>\n",
    "If $M=0$ or set to None, the training is done as Prompt-based FT with input output $x_i\\to y_i$. <br>\n",
    "If $M\\geq1$ then the training is done through In-Context Tuning with $X^{src},x_i\\to y_i$.  Where $X^{src}$ are the context examples drawn from the training dataset.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess and tokenize text\n",
    "\n",
    "M = 10\n",
    "\n",
    "def preprocess_wrapper_icl(sample):\n",
    "    '''\n",
    "    Wrapper for preprocessing each training sample and add context examples if required\n",
    "    '''\n",
    "    return preprocess_function(\n",
    "        sample, \n",
    "        tokenizer, \n",
    "        ict_n = M,\n",
    "        )\n",
    "\n",
    "tokenized_dataset_train = dataset_train.map(\n",
    "    preprocess_wrapper_icl, \n",
    "    batched = True,\n",
    "    remove_columns=[\"text\",'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dca0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data sample:')\n",
    "\n",
    "tokenizer.decode(tokenized_dataset_train['input_ids'][0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73516efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \n",
    "        output_dir = 'checkpoints_trained', #directory to save the checkpoint\n",
    "        learning_rate = 0.0004,\n",
    "        auto_find_batch_size = True,\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 8,\n",
    "        num_train_epochs = 10,\n",
    "        save_strategy = 'epoch',\n",
    "        seed = 1,\n",
    "        data_seed = 1,\n",
    "        ddp_find_unused_parameters = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24045bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_lora(    \n",
    "    base_model = base_model,\n",
    "    peft_training_args = training_args,\n",
    "    dataset_train = tokenized_dataset_train,\n",
    "    lora_config = None, ## to load a LoRA with custom parameters (LoraConfig object)\n",
    "    lora_checkpoint = None, ## provide to continue to fine-tune an already trained LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4542c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
